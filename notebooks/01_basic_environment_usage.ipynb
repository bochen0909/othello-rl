{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Othello RL Environment - Basic Usage\n",
    "\n",
    "This notebook demonstrates the basic usage of the Othello RL environment, including:\n",
    "- Environment creation and configuration\n",
    "- Observation and action spaces\n",
    "- Running episodes with random agents\n",
    "- Action masking\n",
    "- Rendering and visualization\n",
    "- State persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and create an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import aip_rl.othello\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating the Environment\n",
    "\n",
    "Let's create a basic Othello environment with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with default settings\n",
    "env = gym.make(\"Othello-v0\")\n",
    "\n",
    "print(\"Environment created successfully!\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Observation Space\n",
    "\n",
    "The observation is a 3D array with shape (3, 8, 8):\n",
    "- Channel 0: Agent's pieces\n",
    "- Channel 1: Opponent's pieces\n",
    "- Channel 2: Valid moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment and get initial observation\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "print(f\"Observation shape: {observation.shape}\")\n",
    "print(f\"Observation dtype: {observation.dtype}\")\n",
    "print(f\"\\nInfo dictionary keys: {info.keys()}\")\n",
    "print(f\"Initial black count: {info['black_count']}\")\n",
    "print(f\"Initial white count: {info['white_count']}\")\n",
    "print(f\"Number of valid moves: {np.sum(info['action_mask'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Observation Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Agent's pieces (Channel 0)\n",
    "axes[0].imshow(observation[0], cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title(\"Agent's Pieces (Black)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Opponent's pieces (Channel 1)\n",
    "axes[1].imshow(observation[1], cmap='Reds', vmin=0, vmax=1)\n",
    "axes[1].set_title(\"Opponent's Pieces (White)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Valid moves (Channel 2)\n",
    "axes[2].imshow(observation[2], cmap='Greens', vmin=0, vmax=1)\n",
    "axes[2].set_title(\"Valid Moves\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding the Action Space\n",
    "\n",
    "Actions are integers from 0 to 63, representing board positions:\n",
    "- action = row * 8 + col\n",
    "- row = action // 8\n",
    "- col = action % 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show action mapping\n",
    "print(\"Action to position mapping (first few examples):\")\n",
    "for action in [0, 7, 19, 27, 63]:\n",
    "    row = action // 8\n",
    "    col = action % 8\n",
    "    print(f\"Action {action:2d} -> Position ({row}, {col})\")\n",
    "\n",
    "# Show valid actions\n",
    "valid_actions = np.where(info['action_mask'])[0]\n",
    "print(f\"\\nValid actions at start: {valid_actions}\")\n",
    "print(\"Valid positions:\")\n",
    "for action in valid_actions:\n",
    "    row, col = action // 8, action % 8\n",
    "    print(f\"  Action {action} -> ({row}, {col})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running a Single Episode with Random Agent\n",
    "\n",
    "Let's run a complete episode using a random agent that selects from valid moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Track episode statistics\n",
    "episode_rewards = []\n",
    "episode_length = 0\n",
    "done = False\n",
    "\n",
    "print(\"Running episode with random agent...\\n\")\n",
    "\n",
    "while not done:\n",
    "    # Get valid actions from action mask\n",
    "    action_mask = info['action_mask']\n",
    "    valid_actions = np.where(action_mask)[0]\n",
    "    \n",
    "    if len(valid_actions) == 0:\n",
    "        print(\"No valid moves available!\")\n",
    "        break\n",
    "    \n",
    "    # Select random valid action\n",
    "    action = np.random.choice(valid_actions)\n",
    "    \n",
    "    # Take step\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    episode_rewards.append(reward)\n",
    "    episode_length += 1\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Print progress every 5 steps\n",
    "    if episode_length % 5 == 0:\n",
    "        print(f\"Step {episode_length}: Black={info['black_count']}, White={info['white_count']}, Reward={reward:.2f}\")\n",
    "\n",
    "# Episode summary\n",
    "print(f\"\\nEpisode finished!\")\n",
    "print(f\"Episode length: {episode_length}\")\n",
    "print(f\"Total reward: {sum(episode_rewards):.2f}\")\n",
    "print(f\"Final score - Black: {info['black_count']}, White: {info['white_count']}\")\n",
    "\n",
    "# Determine winner\n",
    "if info['black_count'] > info['white_count']:\n",
    "    print(\"Winner: Black (Agent)\")\n",
    "elif info['white_count'] > info['black_count']:\n",
    "    print(\"Winner: White (Opponent)\")\n",
    "else:\n",
    "    print(\"Result: Draw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Reward Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Step (Sparse Reward Mode)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rendering the Game\n",
    "\n",
    "Let's create an environment with rendering enabled and visualize a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with ANSI rendering\n",
    "env_render = gym.make(\"Othello-v0\", render_mode=\"ansi\")\n",
    "observation, info = env_render.reset(seed=42)\n",
    "\n",
    "# Show initial board\n",
    "print(\"Initial board state:\")\n",
    "print(env_render.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a few moves and show board\n",
    "for i in range(5):\n",
    "    action_mask = info['action_mask']\n",
    "    valid_actions = np.where(action_mask)[0]\n",
    "    action = np.random.choice(valid_actions)\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env_render.step(action)\n",
    "    \n",
    "    print(f\"\\nAfter move {i+1} (action {action}):\")\n",
    "    print(env_render.render())\n",
    "    \n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Array Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment with RGB rendering\n",
    "env_rgb = gym.make(\"Othello-v0\", render_mode=\"rgb_array\")\n",
    "observation, info = env_rgb.reset(seed=42)\n",
    "\n",
    "# Get RGB frame\n",
    "rgb_frame = env_rgb.render()\n",
    "\n",
    "print(f\"RGB frame shape: {rgb_frame.shape}\")\n",
    "print(f\"RGB frame dtype: {rgb_frame.dtype}\")\n",
    "\n",
    "# Display the frame\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(rgb_frame)\n",
    "plt.title(\"Initial Board State (RGB Rendering)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Different Reward Modes\n",
    "\n",
    "Let's compare sparse and dense reward modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, seed=42):\n",
    "    \"\"\"Run a single episode and return rewards.\"\"\"\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    rewards = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action_mask = info['action_mask']\n",
    "        valid_actions = np.where(action_mask)[0]\n",
    "        if len(valid_actions) == 0:\n",
    "            break\n",
    "        action = np.random.choice(valid_actions)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Sparse rewards\n",
    "env_sparse = gym.make(\"Othello-v0\", reward_mode=\"sparse\")\n",
    "rewards_sparse = run_episode(env_sparse, seed=42)\n",
    "\n",
    "# Dense rewards\n",
    "env_dense = gym.make(\"Othello-v0\", reward_mode=\"dense\")\n",
    "rewards_dense = run_episode(env_dense, seed=42)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(rewards_sparse)\n",
    "axes[0].set_title('Sparse Rewards')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(rewards_dense)\n",
    "axes[1].set_title('Dense Rewards')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Reward')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sparse - Total reward: {sum(rewards_sparse):.2f}, Non-zero rewards: {np.count_nonzero(rewards_sparse)}\")\n",
    "print(f\"Dense - Total reward: {sum(rewards_dense):.2f}, Non-zero rewards: {np.count_nonzero(rewards_dense)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Different Opponent Policies\n",
    "\n",
    "Let's test different opponent policies: self-play, random, and greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_opponent(opponent_type, num_episodes=10, seed=42):\n",
    "    \"\"\"Evaluate agent against a specific opponent.\"\"\"\n",
    "    env = gym.make(\"Othello-v0\", opponent=opponent_type)\n",
    "    \n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        observation, info = env.reset(seed=seed+i)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action_mask = info['action_mask']\n",
    "            valid_actions = np.where(action_mask)[0]\n",
    "            if len(valid_actions) == 0:\n",
    "                break\n",
    "            action = np.random.choice(valid_actions)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Count results\n",
    "        if info['black_count'] > info['white_count']:\n",
    "            wins += 1\n",
    "        elif info['white_count'] > info['black_count']:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    return wins, losses, draws\n",
    "\n",
    "# Evaluate against different opponents\n",
    "print(\"Evaluating random agent against different opponents...\\n\")\n",
    "\n",
    "for opponent in [\"self\", \"random\", \"greedy\"]:\n",
    "    wins, losses, draws = evaluate_opponent(opponent, num_episodes=10)\n",
    "    total = wins + losses + draws\n",
    "    print(f\"Against {opponent:8s}: Wins={wins}/{total}, Losses={losses}/{total}, Draws={draws}/{total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. State Persistence\n",
    "\n",
    "Demonstrate saving and loading game states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and play some moves\n",
    "env = gym.make(\"Othello-v0\", render_mode=\"ansi\")\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "print(\"Playing 5 moves...\\n\")\n",
    "for i in range(5):\n",
    "    action_mask = info['action_mask']\n",
    "    valid_actions = np.where(action_mask)[0]\n",
    "    action = np.random.choice(valid_actions)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"Board after 5 moves:\")\n",
    "print(env.render())\n",
    "\n",
    "# Save state\n",
    "saved_state = env.save_state()\n",
    "print(f\"\\nState saved! Move history length: {len(saved_state['move_history'])}\")\n",
    "print(f\"Black: {saved_state['black_count']}, White: {saved_state['white_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue playing\n",
    "print(\"\\nPlaying 5 more moves...\\n\")\n",
    "for i in range(5):\n",
    "    action_mask = info['action_mask']\n",
    "    valid_actions = np.where(action_mask)[0]\n",
    "    if len(valid_actions) == 0:\n",
    "        break\n",
    "    action = np.random.choice(valid_actions)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"Board after 10 moves:\")\n",
    "print(env.render())\n",
    "print(f\"Black: {info['black_count']}, White: {info['white_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved state\n",
    "print(\"\\nLoading saved state...\\n\")\n",
    "env.load_state(saved_state)\n",
    "\n",
    "print(\"Board after loading saved state:\")\n",
    "print(env.render())\n",
    "\n",
    "# Verify state was restored\n",
    "current_state = env.save_state()\n",
    "print(f\"\\nState restored successfully!\")\n",
    "print(f\"Move history length: {len(current_state['move_history'])}\")\n",
    "print(f\"Black: {current_state['black_count']}, White: {current_state['white_count']}\")\n",
    "print(f\"Boards match: {np.array_equal(current_state['board'], saved_state['board'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Invalid Move Handling\n",
    "\n",
    "Demonstrate different invalid move handling modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalty mode (default)\n",
    "env_penalty = gym.make(\"Othello-v0\", invalid_move_mode=\"penalty\", invalid_move_penalty=-5.0)\n",
    "observation, info = env_penalty.reset(seed=42)\n",
    "\n",
    "# Try an invalid move (position 0 is not valid at start)\n",
    "invalid_action = 0\n",
    "print(f\"Attempting invalid action {invalid_action}...\")\n",
    "print(f\"Is action valid? {info['action_mask'][invalid_action]}\")\n",
    "\n",
    "observation, reward, terminated, truncated, info = env_penalty.step(invalid_action)\n",
    "print(f\"Reward received: {reward}\")\n",
    "print(f\"Game terminated: {terminated}\")\n",
    "print(f\"Game continues after invalid move in penalty mode.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random mode - automatically selects valid move\n",
    "env_random = gym.make(\"Othello-v0\", invalid_move_mode=\"random\")\n",
    "observation, info = env_random.reset(seed=42)\n",
    "\n",
    "print(f\"Attempting invalid action {invalid_action} in random mode...\")\n",
    "observation, reward, terminated, truncated, info = env_random.step(invalid_action)\n",
    "print(f\"Reward received: {reward}\")\n",
    "print(f\"A valid move was automatically selected instead.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error mode - raises exception\n",
    "env_error = gym.make(\"Othello-v0\", invalid_move_mode=\"error\")\n",
    "observation, info = env_error.reset(seed=42)\n",
    "\n",
    "print(f\"Attempting invalid action {invalid_action} in error mode...\")\n",
    "try:\n",
    "    observation, reward, terminated, truncated, info = env_error.step(invalid_action)\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError raised: {e}\")\n",
    "    print(\"Error mode raises exception for invalid moves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "1. Creating and configuring the Othello environment\n",
    "2. Understanding observation and action spaces\n",
    "3. Running episodes with random agents\n",
    "4. Using action masking for valid moves\n",
    "5. Different rendering modes (ANSI, RGB)\n",
    "6. Comparing reward modes (sparse vs dense)\n",
    "7. Testing different opponent policies\n",
    "8. Saving and loading game states\n",
    "9. Handling invalid moves\n",
    "\n",
    "Next steps:\n",
    "- See `02_training_with_rllib.ipynb` for training RL agents\n",
    "- See `03_evaluating_trained_agents.ipynb` for evaluation and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
