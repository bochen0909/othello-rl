{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Othello Agents with Ray RLlib\n",
    "\n",
    "This notebook demonstrates how to train reinforcement learning agents to play Othello using Ray RLlib, including:\n",
    "- Setting up RLlib with the Othello environment\n",
    "- Configuring PPO algorithm\n",
    "- Creating custom CNN models\n",
    "- Training agents with self-play\n",
    "- Monitoring training progress\n",
    "- Saving and loading checkpoints\n",
    "- Vectorized environments for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import aip_rl.othello\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic PPO Training\n",
    "\n",
    "Let's start with a simple PPO training setup using default models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Configure PPO\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"Othello-v0\",\n",
    "        env_config={\n",
    "            \"opponent\": \"self\",\n",
    "            \"reward_mode\": \"sparse\",\n",
    "            \"invalid_move_mode\": \"penalty\",\n",
    "        }\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=2,  # Number of parallel workers\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=0,  # Set to 1 if GPU available\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        minibatch_size=128,\n",
    "        num_sgd_iter=10,\n",
    "        lr=0.0003,\n",
    "        gamma=0.99,\n",
    "        lambda_=0.95,\n",
    "        clip_param=0.2,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"PPO configuration created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build algorithm\n",
    "algo = config.build()\n",
    "\n",
    "print(\"Algorithm built successfully!\")\n",
    "print(f\"Algorithm: {algo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few iterations\n",
    "num_iterations = 10\n",
    "results = []\n",
    "\n",
    "print(f\"Training for {num_iterations} iterations...\\n\")\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    result = algo.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print progress\n",
    "    if \"env_runners\" in result:\n",
    "        episode_return = result[\"env_runners\"].get(\"episode_return_mean\", 0)\n",
    "        episode_len = result[\"env_runners\"].get(\"episode_len_mean\", 0)\n",
    "        print(f\"Iteration {i+1:2d}: Reward={episode_return:6.2f}, Length={episode_len:5.1f}\")\n",
    "    else:\n",
    "        print(f\"Iteration {i+1:2d}: Training...\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics\n",
    "episode_returns = []\n",
    "episode_lengths = []\n",
    "\n",
    "for result in results:\n",
    "    if \"env_runners\" in result:\n",
    "        episode_returns.append(result[\"env_runners\"].get(\"episode_return_mean\", 0))\n",
    "        episode_lengths.append(result[\"env_runners\"].get(\"episode_len_mean\", 0))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(episode_returns)\n",
    "axes[0].set_title('Episode Return Mean')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Return')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(episode_lengths)\n",
    "axes[1].set_title('Episode Length Mean')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Length')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom CNN Model\n",
    "\n",
    "For better performance with the (3, 8, 8) observation space, let's create a custom CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloCNN(TorchModelV2, nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN model for Othello board observations.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv2d(3, 64, 3x3) + ReLU\n",
    "    - Conv2d(64, 128, 3x3) + ReLU\n",
    "    - Conv2d(128, 128, 3x3) + ReLU\n",
    "    - Flatten\n",
    "    - Linear(128*8*8, 512) + ReLU\n",
    "    - Linear(512, num_outputs) for policy\n",
    "    - Linear(512, 1) for value function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, \n",
    "                             model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        # CNN layers for (3, 8, 8) input\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_outputs)\n",
    "        \n",
    "        # Value function head\n",
    "        self.value_fc = nn.Linear(512, 1)\n",
    "        \n",
    "        self._features = None\n",
    "    \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        x = input_dict[\"obs\"].float()\n",
    "        \n",
    "        # CNN forward pass\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        self._features = x\n",
    "        \n",
    "        # Policy logits\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        return logits, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self.value_fc(self._features).squeeze(1)\n",
    "\n",
    "# Register custom model\n",
    "ModelCatalog.register_custom_model(\"othello_cnn\", OthelloCNN)\n",
    "\n",
    "print(\"Custom CNN model registered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop previous algorithm\n",
    "algo.stop()\n",
    "\n",
    "# Configure PPO with custom model\n",
    "config_cnn = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"Othello-v0\",\n",
    "        env_config={\n",
    "            \"opponent\": \"self\",\n",
    "            \"reward_mode\": \"sparse\",\n",
    "        }\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .resources(num_gpus=0)\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        lr=0.0003,\n",
    "    )\n",
    "    .model({\n",
    "        \"custom_model\": \"othello_cnn\",\n",
    "    })\n",
    ")\n",
    "\n",
    "# Build and train\n",
    "algo_cnn = config_cnn.build()\n",
    "\n",
    "print(\"Training with custom CNN model...\\n\")\n",
    "\n",
    "results_cnn = []\n",
    "for i in range(10):\n",
    "    result = algo_cnn.train()\n",
    "    results_cnn.append(result)\n",
    "    \n",
    "    if \"env_runners\" in result:\n",
    "        episode_return = result[\"env_runners\"].get(\"episode_return_mean\", 0)\n",
    "        episode_len = result[\"env_runners\"].get(\"episode_len_mean\", 0)\n",
    "        print(f\"Iteration {i+1:2d}: Reward={episode_return:6.2f}, Length={episode_len:5.1f}\")\n",
    "\n",
    "print(\"\\nTraining with CNN complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorized Environments\n",
    "\n",
    "Use multiple parallel environments for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop previous algorithm\n",
    "algo_cnn.stop()\n",
    "\n",
    "# Configure with vectorized environments\n",
    "config_vec = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"Othello-v0\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=4,           # 4 parallel workers\n",
    "        num_envs_per_env_runner=4,   # 4 environments per worker\n",
    "    )\n",
    "    .resources(num_gpus=0)\n",
    "    .training(\n",
    "        train_batch_size=8000,\n",
    "        lr=0.0003,\n",
    "    )\n",
    "    .model({\"custom_model\": \"othello_cnn\"})\n",
    ")\n",
    "\n",
    "print(\"Configuration with 4 * 4 = 16 parallel environments\")\n",
    "print(\"This significantly speeds up training!\\n\")\n",
    "\n",
    "algo_vec = config_vec.build()\n",
    "\n",
    "print(\"Training with vectorized environments...\\n\")\n",
    "\n",
    "results_vec = []\n",
    "for i in range(10):\n",
    "    result = algo_vec.train()\n",
    "    results_vec.append(result)\n",
    "    \n",
    "    if \"env_runners\" in result:\n",
    "        episode_return = result[\"env_runners\"].get(\"episode_return_mean\", 0)\n",
    "        episode_len = result[\"env_runners\"].get(\"episode_len_mean\", 0)\n",
    "        print(f\"Iteration {i+1:2d}: Reward={episode_return:6.2f}, Length={episode_len:5.1f}\")\n",
    "\n",
    "print(\"\\nVectorized training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checkpointing\n",
    "\n",
    "Save and load model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "checkpoint_path = algo_vec.save()\n",
    "print(f\"Checkpoint saved to: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop current algorithm\n",
    "algo_vec.stop()\n",
    "\n",
    "# Create new algorithm and restore from checkpoint\n",
    "algo_restored = config_vec.build()\n",
    "algo_restored.restore(checkpoint_path)\n",
    "\n",
    "print(f\"Algorithm restored from checkpoint!\")\n",
    "print(f\"Continuing training...\\n\")\n",
    "\n",
    "# Continue training\n",
    "for i in range(5):\n",
    "    result = algo_restored.train()\n",
    "    \n",
    "    if \"env_runners\" in result:\n",
    "        episode_return = result[\"env_runners\"].get(\"episode_return_mean\", 0)\n",
    "        print(f\"Iteration {i+1}: Reward={episode_return:.2f}\")\n",
    "\n",
    "print(\"\\nContinued training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Trained Agent\n",
    "\n",
    "Let's test the trained agent in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test environment\n",
    "env = gym.make(\"Othello-v0\", render_mode=\"ansi\")\n",
    "\n",
    "# Run test episode\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "\n",
    "print(\"Testing trained agent...\\n\")\n",
    "print(\"Initial board:\")\n",
    "print(env.render())\n",
    "\n",
    "while not done and step_count < 60:\n",
    "    # Get action from trained policy\n",
    "    action = algo_restored.compute_single_action(observation)\n",
    "    \n",
    "    # Take step\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Print every 10 steps\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"\\nAfter {step_count} steps:\")\n",
    "        print(env.render())\n",
    "\n",
    "print(f\"\\nFinal board:\")\n",
    "print(env.render())\n",
    "print(f\"\\nEpisode finished!\")\n",
    "print(f\"Steps: {step_count}\")\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "print(f\"Final score - Black: {info['black_count']}, White: {info['white_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Different Reward Modes\n",
    "\n",
    "Compare training with sparse vs dense rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop current algorithm\n",
    "algo_restored.stop()\n",
    "\n",
    "# Train with dense rewards\n",
    "config_dense = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"Othello-v0\",\n",
    "        env_config={\n",
    "            \"opponent\": \"self\",\n",
    "            \"reward_mode\": \"dense\",  # Dense rewards\n",
    "        }\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .resources(num_gpus=0)\n",
    "    .training(train_batch_size=4000, lr=0.0003)\n",
    "    .model({\"custom_model\": \"othello_cnn\"})\n",
    ")\n",
    "\n",
    "algo_dense = config_dense.build()\n",
    "\n",
    "print(\"Training with dense rewards...\\n\")\n",
    "\n",
    "results_dense = []\n",
    "for i in range(10):\n",
    "    result = algo_dense.train()\n",
    "    results_dense.append(result)\n",
    "    \n",
    "    if \"env_runners\" in result:\n",
    "        episode_return = result[\"env_runners\"].get(\"episode_return_mean\", 0)\n",
    "        print(f\"Iteration {i+1:2d}: Reward={episode_return:6.2f}\")\n",
    "\n",
    "print(\"\\nDense reward training complete!\")\n",
    "\n",
    "# Compare learning curves\n",
    "sparse_returns = [r[\"env_runners\"].get(\"episode_return_mean\", 0) for r in results_vec if \"env_runners\" in r]\n",
    "dense_returns = [r[\"env_runners\"].get(\"episode_return_mean\", 0) for r in results_dense if \"env_runners\" in r]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sparse_returns, label='Sparse Rewards', marker='o')\n",
    "plt.plot(dense_returns, label='Dense Rewards', marker='s')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Episode Return Mean')\n",
    "plt.title('Sparse vs Dense Rewards')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "algo_dense.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "Evaluate the trained agent over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(algo, num_episodes=20):\n",
    "    \"\"\"Evaluate agent over multiple episodes.\"\"\"\n",
    "    env = gym.make(\"Othello-v0\")\n",
    "    \n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    total_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = algo.compute_single_action(observation)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        # Count results\n",
    "        if info['black_count'] > info['white_count']:\n",
    "            wins += 1\n",
    "        elif info['white_count'] > info['black_count']:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    return {\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'draws': draws,\n",
    "        'mean_reward': np.mean(total_rewards),\n",
    "        'std_reward': np.std(total_rewards),\n",
    "        'mean_length': np.mean(episode_lengths),\n",
    "    }\n",
    "\n",
    "# Note: This requires a trained algorithm to be active\n",
    "# Uncomment and run if you have a trained algorithm\n",
    "# results = evaluate_agent(algo_restored, num_episodes=20)\n",
    "# print(f\"Evaluation Results:\")\n",
    "# print(f\"  Wins: {results['wins']}/20\")\n",
    "# print(f\"  Losses: {results['losses']}/20\")\n",
    "# print(f\"  Draws: {results['draws']}/20\")\n",
    "# print(f\"  Mean Reward: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}\")\n",
    "# print(f\"  Mean Episode Length: {results['mean_length']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Shutdown Ray when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray\n",
    "ray.shutdown()\n",
    "print(\"Ray shutdown complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "1. Basic PPO training with default models\n",
    "2. Creating and using custom CNN models\n",
    "3. Vectorized environments for faster training\n",
    "4. Saving and loading checkpoints\n",
    "5. Testing trained agents\n",
    "6. Comparing sparse vs dense rewards\n",
    "7. Evaluating agent performance\n",
    "\n",
    "Key takeaways:\n",
    "- Custom CNN models improve performance for board games\n",
    "- Vectorized environments significantly speed up training\n",
    "- Dense rewards can provide faster initial learning\n",
    "- Regular checkpointing is important for long training runs\n",
    "\n",
    "Next steps:\n",
    "- See `03_evaluating_trained_agents.ipynb` for detailed evaluation and analysis\n",
    "- Experiment with different hyperparameters\n",
    "- Try other RLlib algorithms (DQN, APPO, etc.)\n",
    "- Implement curriculum learning or opponent diversity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
