{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Trained Othello Agents\n",
    "\n",
    "This notebook demonstrates how to evaluate and analyze trained Othello agents, including:\n",
    "- Loading trained models from checkpoints\n",
    "- Evaluating against different opponents\n",
    "- Analyzing game statistics and patterns\n",
    "- Visualizing agent behavior\n",
    "- Creating game replays\n",
    "- Comparing multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import aip_rl.othello\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a Trained Agent\n",
    "\n",
    "First, let's load a trained agent from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Path to your checkpoint (update this with your actual checkpoint path)\n",
    "checkpoint_path = \"/path/to/your/checkpoint\"\n",
    "\n",
    "# Note: For this demo, we'll train a quick agent\n",
    "# In practice, you would load a pre-trained checkpoint\n",
    "print(\"For this demo, we'll train a quick agent...\")\n",
    "print(\"In practice, you would load a pre-trained checkpoint.\\n\")\n",
    "\n",
    "# Quick training for demo purposes\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"Othello-v0\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .resources(num_gpus=0)\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "# Train for a few iterations\n",
    "for i in range(5):\n",
    "    result = algo.train()\n",
    "    print(f\"Training iteration {i+1}/5...\")\n",
    "\n",
    "print(\"\\nAgent ready for evaluation!\")"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Against Different Opponents\n",
    "\n",
    "Let's evaluate the agent against random, greedy, and self-play opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vs_opponent(algo, opponent_type, num_episodes=20, seed=42):\n",
    "    \"\"\"\n",
    "    Evaluate agent against a specific opponent.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with wins, losses, draws, and statistics\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Othello-v0\", opponent=opponent_type)\n",
    "    \n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "    piece_diffs = []  # Final piece count difference\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        observation, info = env.reset(seed=seed + episode)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = algo.compute_single_action(observation)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        lengths.append(steps)\n",
    "        \n",
    "        # Determine winner and piece difference\n",
    "        black_count = info['black_count']\n",
    "        white_count = info['white_count']\n",
    "        piece_diff = black_count - white_count  # Agent is Black\n",
    "        piece_diffs.append(piece_diff)\n",
    "        \n",
    "        if black_count > white_count:\n",
    "            wins += 1\n",
    "        elif white_count > black_count:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    return {\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'draws': draws,\n",
    "        'win_rate': wins / num_episodes,\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'mean_length': np.mean(lengths),\n",
    "        'mean_piece_diff': np.mean(piece_diffs),\n",
    "        'rewards': rewards,\n",
    "        'lengths': lengths,\n",
    "        'piece_diffs': piece_diffs,\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  }
,
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against different opponents\n",
    "opponents = [\"random\", \"greedy\"]\n",
    "results = {}\n",
    "\n",
    "print(\"Evaluating agent against different opponents...\\n\")\n",
    "\n",
    "for opponent in opponents:\n",
    "    print(f\"Evaluating vs {opponent}...\")\n",
    "    results[opponent] = evaluate_vs_opponent(algo, opponent, num_episodes=20)\n",
    "    \n",
    "    r = results[opponent]\n",
    "    print(f\"  Wins: {r['wins']}/20 ({r['win_rate']*100:.1f}%)\")\n",
    "    print(f\"  Losses: {r['losses']}/20\")\n",
    "    print(f\"  Draws: {r['draws']}/20\")\n",
    "    print(f\"  Mean Reward: {r['mean_reward']:.2f} ± {r['std_reward']:.2f}\")\n",
    "    print(f\"  Mean Episode Length: {r['mean_length']:.1f}\")\n",
    "    print(f\"  Mean Piece Difference: {r['mean_piece_diff']:.1f}\\n\")\n",
    "\n",
    "print(\"Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Win rates\n",
    "ax = axes[0, 0]\n",
    "opponent_names = list(results.keys())\n",
    "win_rates = [results[opp]['win_rate'] * 100 for opp in opponent_names]\n",
    "ax.bar(opponent_names, win_rates, color=['skyblue', 'lightcoral'])\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title('Win Rate vs Different Opponents')\n",
    "ax.set_ylim([0, 100])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Mean rewards\n",
    "ax = axes[0, 1]\n",
    "mean_rewards = [results[opp]['mean_reward'] for opp in opponent_names]\n",
    "std_rewards = [results[opp]['std_reward'] for opp in opponent_names]\n",
    "ax.bar(opponent_names, mean_rewards, yerr=std_rewards, \n",
    "       color=['skyblue', 'lightcoral'], capsize=5)\n",
    "ax.set_ylabel('Mean Reward')\n",
    "ax.set_title('Mean Reward vs Different Opponents')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Episode lengths\n",
    "ax = axes[1, 0]\n",
    "for opponent in opponent_names:\n",
    "    ax.hist(results[opponent]['lengths'], alpha=0.6, label=opponent, bins=10)\n",
    "ax.set_xlabel('Episode Length')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Episode Length Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Piece differences\n",
    "ax = axes[1, 1]\n",
    "for opponent in opponent_names:\n",
    "    ax.hist(results[opponent]['piece_diffs'], alpha=0.6, label=opponent, bins=15)\n",
    "ax.set_xlabel('Piece Difference (Agent - Opponent)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Final Piece Difference Distribution')\n",
    "ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Game Replay and Visualization\n",
    "\n",
    "Let's record and visualize a complete game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_game(algo, opponent=\"random\", seed=42):\n",
    "    \"\"\"\n",
    "    Record a complete game with all states and actions.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Othello-v0\", opponent=opponent, render_mode=\"rgb_array\")\n",
    "    \n",
    "    observation, info = env.reset(seed=seed)\n",
    "    \n",
    "    game_record = {\n",
    "        'observations': [observation.copy()],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "        'infos': [info.copy()],\n",
    "        'frames': [env.render()],\n",
    "    }\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = algo.compute_single_action(observation)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        game_record['actions'].append(action)\n",
    "        game_record['rewards'].append(reward)\n",
    "        game_record['observations'].append(observation.copy())\n",
    "        game_record['infos'].append(info.copy())\n",
    "        game_record['frames'].append(env.render())\n",
    "        \n",
    "        done = terminated or truncated\n",
    "    \n",
    "    return game_record\n",
    "\n",
    "# Record a game\n",
    "print(\"Recording a game...\")\n",
    "game = record_game(algo, opponent=\"greedy\", seed=42)\n",
    "print(f\"Game recorded! {len(game['actions'])} moves played.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key moments from the game\n",
    "key_moments = [0, len(game['frames'])//4, len(game['frames'])//2, \n",
    "               3*len(game['frames'])//4, -1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for idx, moment in enumerate(key_moments):\n",
    "    axes[idx].imshow(game['frames'][moment])\n",
    "    step = moment if moment >= 0 else len(game['frames']) + moment\n",
    "    info = game['infos'][moment]\n",
    "    axes[idx].set_title(f\"Step {step}\\nB:{info['black_count']} W:{info['white_count']}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Key Moments in the Game', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Agent Behavior\n",
    "\n",
    "Let's analyze the agent's decision-making patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze action distribution\n",
    "action_counts = defaultdict(int)\n",
    "for action in game['actions']:\n",
    "    action_counts[action] += 1\n",
    "\n",
    "# Convert actions to board positions\n",
    "position_heatmap = np.zeros((8, 8))\n",
    "for action, count in action_counts.items():\n",
    "    row, col = action // 8, action % 8\n",
    "    position_heatmap[row, col] = count\n",
    "\n",
    "# Visualize action heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(position_heatmap, cmap='YlOrRd', interpolation='nearest')\n",
    "plt.colorbar(label='Number of times played')\n",
    "plt.title('Agent Action Heatmap\\n(Darker = More Frequently Played)')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "\n",
    "# Add grid\n",
    "for i in range(9):\n",
    "    plt.axhline(i-0.5, color='gray', linewidth=0.5)\n",
    "    plt.axvline(i-0.5, color='gray', linewidth=0.5)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if position_heatmap[i, j] > 0:\n",
    "            plt.text(j, i, int(position_heatmap[i, j]), \n",
    "                    ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMost played positions:\")\n",
    "sorted_actions = sorted(action_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for action, count in sorted_actions:\n",
    "    row, col = action // 8, action % 8\n",
    "    print(f\"  Position ({row}, {col}): {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward progression\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(game['rewards'], marker='o', markersize=3)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Progression Throughout Game')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total reward: {sum(game['rewards']):.2f}\")\n",
    "print(f\"Final result: Black {game['infos'][-1]['black_count']} - {game['infos'][-1]['white_count']} White\")"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "Perform statistical analysis of agent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect statistics from multiple games\n",
    "def collect_statistics(algo, opponent=\"random\", num_games=50):\n",
    "    \"\"\"Collect detailed statistics from multiple games.\"\"\"\n",
    "    stats = {\n",
    "        'game_lengths': [],\n",
    "        'final_scores': [],\n",
    "        'piece_diffs': [],\n",
    "        'outcomes': [],  # 'win', 'loss', 'draw'\n",
    "        'total_rewards': [],\n",
    "    }\n",
    "    \n",
    "    env = gym.make(\"Othello-v0\", opponent=opponent)\n",
    "    \n",
    "    for game_idx in range(num_games):\n",
    "        observation, info = env.reset(seed=42 + game_idx)\n",
    "        done = False\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = algo.compute_single_action(observation)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Record statistics\n",
    "        black_count = info['black_count']\n",
    "        white_count = info['white_count']\n",
    "        \n",
    "        stats['game_lengths'].append(steps)\n",
    "        stats['final_scores'].append((black_count, white_count))\n",
    "        stats['piece_diffs'].append(black_count - white_count)\n",
    "        stats['total_rewards'].append(total_reward)\n",
    "        \n",
    "        if black_count > white_count:\n",
    "            stats['outcomes'].append('win')\n",
    "        elif white_count > black_count:\n",
    "            stats['outcomes'].append('loss')\n",
    "        else:\n",
    "            stats['outcomes'].append('draw')\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"Collecting statistics from 50 games vs random opponent...\")\n",
    "stats = collect_statistics(algo, opponent=\"random\", num_games=50)\n",
    "print(\"Statistics collected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze statistics\n",
    "wins = stats['outcomes'].count('win')\n",
    "losses = stats['outcomes'].count('loss')\n",
    "draws = stats['outcomes'].count('draw')\n",
    "\n",
    "print(\"\\n=== Performance Summary ===\")\n",
    "print(f\"Games played: {len(stats['outcomes'])}\")\n",
    "print(f\"Wins: {wins} ({wins/len(stats['outcomes'])*100:.1f}%)\")\n",
    "print(f\"Losses: {losses} ({losses/len(stats['outcomes'])*100:.1f}%)\")\n",
    "print(f\"Draws: {draws} ({draws/len(stats['outcomes'])*100:.1f}%)\")\n",
    "print(f\"\\nMean game length: {np.mean(stats['game_lengths']):.1f} ± {np.std(stats['game_lengths']):.1f}\")\n",
    "print(f\"Mean piece difference: {np.mean(stats['piece_diffs']):.1f} ± {np.std(stats['piece_diffs']):.1f}\")\n",
    "print(f\"Mean total reward: {np.mean(stats['total_rewards']):.2f} ± {np.std(stats['total_rewards']):.2f}\")\n",
    "\n",
    "# Winning margin analysis\n",
    "win_margins = [diff for diff, outcome in zip(stats['piece_diffs'], stats['outcomes']) if outcome == 'win']\n",
    "loss_margins = [abs(diff) for diff, outcome in zip(stats['piece_diffs'], stats['outcomes']) if outcome == 'loss']\n",
    "\n",
    "if win_margins:\n",
    "    print(f\"\\nAverage winning margin: {np.mean(win_margins):.1f} pieces\")\n",
    "if loss_margins:\n",
    "    print(f\"Average losing margin: {np.mean(loss_margins):.1f} pieces\")"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive statistics visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Outcome distribution\n",
    "ax = axes[0, 0]\n",
    "outcome_counts = [wins, losses, draws]\n",
    "colors = ['green', 'red', 'gray']\n",
    "ax.pie(outcome_counts, labels=['Wins', 'Losses', 'Draws'], \n",
    "       autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax.set_title('Game Outcomes')\n",
    "\n",
    "# Game length distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(stats['game_lengths'], bins=15, color='skyblue', edgecolor='black')\n",
    "ax.axvline(np.mean(stats['game_lengths']), color='red', \n",
    "          linestyle='--', label=f\"Mean: {np.mean(stats['game_lengths']):.1f}\")\n",
    "ax.set_xlabel('Game Length')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Game Length Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Piece difference distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(stats['piece_diffs'], bins=20, color='lightcoral', edgecolor='black')\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=2, label='Draw line')\n",
    "ax.axvline(np.mean(stats['piece_diffs']), color='red', \n",
    "          linestyle='--', label=f\"Mean: {np.mean(stats['piece_diffs']):.1f}\")\n",
    "ax.set_xlabel('Piece Difference (Agent - Opponent)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Final Piece Difference Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Reward distribution\n",
    "ax = axes[1, 1]\n",
    "ax.hist(stats['total_rewards'], bins=15, color='lightgreen', edgecolor='black')\n",
    "ax.axvline(np.mean(stats['total_rewards']), color='red', \n",
    "          linestyle='--', label=f\"Mean: {np.mean(stats['total_rewards']):.2f}\")\n",
    "ax.set_xlabel('Total Reward')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Total Reward Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop algorithm and shutdown Ray\n",
    "algo.stop()\n",
    "ray.shutdown()\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "1. Loading trained agents from checkpoints\n",
    "2. Evaluating against different opponent types\n",
    "3. Recording and visualizing complete games\n",
    "4. Analyzing agent behavior and decision patterns\n",
    "5. Statistical analysis of performance\n",
    "6. Comprehensive visualization of results\n",
    "\n",
    "Key insights:\n",
    "- Win rate and margin analysis help understand agent strength\n",
    "- Action heatmaps reveal strategic preferences\n",
    "- Game length and piece difference distributions show consistency\n",
    "- Comparing against multiple opponents provides robust evaluation\n",
    "\n",
    "Next steps:\n",
    "- Compare multiple trained agents\n",
    "- Analyze learning progression over training\n",
    "- Implement tournament-style evaluation\n",
    "- Study specific game positions and tactics\n",
    "- Create interactive visualization tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
